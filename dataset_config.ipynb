{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78d29a6d-8993-4286-87b4-c6541931351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6178414-fd97-4156-bc8d-8155e1fb2684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e74c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk('/home/serfani/serfani_data0/E33OMA'):\n",
    "    \n",
    "    sorted_files = sorted(files)\n",
    "    list1 = [os.path.join(root, file) for file in sorted_files if file.split(\".\")[1] == 'aijlh1E33oma_ai']   # Velocity Fields (time, level, lat, lon)\n",
    "    list2 = [os.path.join(root, file) for file in sorted_files if file.split(\".\")[1] == 'cijh1E33oma_ai']    # Precipitation (time, lat, lon)\n",
    "    list3 = [os.path.join(root, file) for file in sorted_files if file.split(\".\")[1] == 'taijh1E33oma_ai']   # Sea Salt Src (time, lat, lon)\n",
    "    list4 = [os.path.join(root, file) for file in sorted_files if file.split(\".\")[1] == 'tNDaijh1E33oma_ai'] # Clay-BCB Src (time, lat, lon)\n",
    "    list5 = [os.path.join(root, file) for file in sorted_files if file.split(\".\")[1] == 'taijlh1E33oma_ai']  # Aerosols Mixing Ratio (time, level, lat, lon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acab7a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = list()\n",
    "ds1  = xr.open_mfdataset(list1[:365]).isel(level=0)\n",
    "data1.append([ds1['u'].mean().values.item(), ds1['u'].std().values.item(), ds1['u'].min().values.item(), ds1['u'].max().values.item()])\n",
    "data1.append([ds1['v'].mean().values.item(), ds1['v'].std().values.item(), ds1['v'].min().values.item(), ds1['v'].max().values.item()])\n",
    "data1.append([ds1['omega'].mean().values.item(), ds1['omega'].std().values.item(), ds1['omega'].min().values.item(), ds1['omega'].max().values.item()])\n",
    "\n",
    "ds2  = xr.open_mfdataset(list2[:365])\n",
    "data1.append([ds2['prec'].mean().values.item(), ds2['prec'].std().values.item(), ds2['prec'].min().values.item(), ds2['prec'].max().values.item()])\n",
    "\n",
    "ds3  = xr.open_mfdataset(list3[:365])\n",
    "data1.append([ds3['seasalt1_ocean_src'].mean().values.item(), ds3['seasalt1_ocean_src'].std().values.item(), ds3['seasalt1_ocean_src'].min().values.item(), ds3['seasalt1_ocean_src'].max().values.item()])\n",
    "\n",
    "ds4  = xr.open_mfdataset(list4[:365])\n",
    "data1.append([ds4['Clay_emission'].mean().values.item(), ds4['Clay_emission'].std().values.item(), ds4['Clay_emission'].min().values.item(), ds4['Clay_emission'].max().values.item()])\n",
    "data1.append([ds4['BCB_biomass_src'].mean().values.item(), ds4['BCB_biomass_src'].std().values.item(), ds4['BCB_biomass_src'].min().values.item(), ds4['BCB_biomass_src'].max().values.item()])\n",
    "\n",
    "ds5  = xr.open_mfdataset(list5[:365]).isel(level=0)\n",
    "data1.append([ds5['seasalt1'].mean().values.item(), ds5['seasalt1'].std().values.item(), ds5['seasalt1'].min().values.item(), ds5['seasalt1'].max().values.item()])\n",
    "data1.append([ds5['Clay'].mean().values.item(), ds5['Clay'].std().values.item(), ds5['Clay'].min().values.item(), ds5['Clay'].max().values.item()])\n",
    "data1.append([ds5['BCB'].mean().values.item(), ds5['BCB'].std().values.item(), ds5['BCB'].min().values.item(), ds5['BCB'].max().values.item()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c559a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = list()\n",
    "ds1  = xr.open_mfdataset(list1[:365]).isel(level=0)\n",
    "u = np.array(np.ma.log10(ds1['u'].values))\n",
    "data2.append([u.mean(), u.std(), u.min(), u.max()])\n",
    "\n",
    "v = np.array(np.ma.log10(ds1['v'].values))\n",
    "data2.append([v.mean(), v.std(), v.min(), v.max()])\n",
    "\n",
    "w = np.array(np.ma.log10(ds1['omega'].values))\n",
    "data2.append([w.mean(), w.std(), w.min(), w.max()])\n",
    "\n",
    "\n",
    "ds2  = xr.open_mfdataset(list2[:365])\n",
    "p = np.array(np.ma.log10(ds2['prec'].values))\n",
    "data2.append([p.mean(), p.std(), p.min(), p.max()])\n",
    "\n",
    "\n",
    "ds3  = xr.open_mfdataset(list3[:365])\n",
    "ss_src = np.array(np.ma.log10(ds3['seasalt1_ocean_src'].values))\n",
    "data2.append([ss_src.mean(), ss_src.std(), ss_src.min(), ss_src.max()])\n",
    "\n",
    "\n",
    "ds4  = xr.open_mfdataset(list4[:365])\n",
    "c_src = np.array(np.ma.log10(ds4['Clay_emission'].values))\n",
    "data2.append([c_src.mean(), c_src.std(), c_src.min(), c_src.max()])\n",
    "\n",
    "bc_src = np.array(np.ma.log10(ds4['BCB_biomass_src'].values))\n",
    "data2.append([bc_src.mean(), bc_src.std(), bc_src.min(), bc_src.max()])\n",
    "\n",
    "\n",
    "ds5  = xr.open_mfdataset(list5[:365]).isel(level=0)\n",
    "ss_conc = np.array(np.ma.log10(ds5['seasalt1'].values))\n",
    "data2.append([ss_conc.mean(), ss_conc.std(), ss_conc.min(), ss_conc.max()])\n",
    "\n",
    "c_conc= np.array(np.ma.log10(ds5['Clay'].values))\n",
    "data2.append([c_conc.mean(), c_conc.std(), c_conc.min(), c_conc.max()])\n",
    "\n",
    "bc_conc = np.array(np.ma.log10(ds5['BCB'].values))\n",
    "data2.append([bc_conc.mean(), bc_conc.std(), bc_conc.min(), bc_conc.max()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "918af8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = [data1, data2]\n",
    "\n",
    "variables = ['u', 'v', 'w', 'prec', 'ss_src', 'c_src', 'bc_src', 'ss_conc', 'c_conc', 'bc_conc']\n",
    "\n",
    "result = {'set1': {}, 'set2': {}}\n",
    "\n",
    "for idx1, key in enumerate(result.keys()):\n",
    "    for idx2, variable in enumerate(variables):\n",
    "        mean, std, min_val, max_val = data[idx1][idx2]\n",
    "        result[key][variable] = {\n",
    "            'mean': float(mean),\n",
    "            'std':  float(std),\n",
    "            'min':  float(min_val),\n",
    "            'max':  float(max_val)\n",
    "        }\n",
    "\n",
    "with open('variable_statistics.json', 'a') as jf:\n",
    "    json.dump(result, jf, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c976018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", message=\"Converting a CFTimeIndex with dates from a non-standard calendar\")\n",
    "    for root, dirs, files in os.walk('/home/serfani/serfani_data0/E33OMA'):\n",
    "        \n",
    "        sorted_files = sorted(files)\n",
    "        list1 = [os.path.join(root, file) for file in sorted_files if file.split(\".\")[1] == 'aijlh1E33oma_ai']   # Velocity Fields (time, level, lat, lon)\n",
    "\n",
    "    datetimeindex = xr.open_mfdataset(list1[:365]).indexes['time'].to_datetimeindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a796a023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "import warnings\n",
    "\n",
    "\n",
    "class E33OMA(Dataset):\n",
    "\n",
    "    def __init__(self, period, species, padding, root='/home/serfani/serfani_data0/E33OMA'):\n",
    "        super(E33OMA, self).__init__()\n",
    "        \n",
    "        self.period  = period\n",
    "        self.species = species\n",
    "        self.padding = padding\n",
    "        self.root    = root\n",
    "        \n",
    "        self._get_data_index()\n",
    "    \n",
    "    def _get_data_index(self):\n",
    "        \n",
    "        for root, dirs, files in os.walk(self.root):\n",
    "            \n",
    "            sorted_files = sorted(files)\n",
    "            list1 = [os.path.join(root, file) for file in sorted_files if file.split(\".\")[1] == 'aijlh1E33oma_ai']   # Velocity Fields (time, level, lat, lon)\n",
    "            # list2 = [os.path.join(root, file) for file in sorted_files if file.split(\".\")[1] == 'cijh1E33oma_ai']    # Precipitation (time, lat, lon)\n",
    "            # list3 = [os.path.join(root, file) for file in sorted_files if file.split(\".\")[1] == 'taijh1E33oma_ai']   # Sea Salt Src (time, lat, lon)\n",
    "            # list3 = [os.path.join(root, file) for file in sorted_files if file.split(\".\")[1] == 'tNDaijh1E33oma_ai'] # Clay-BCB Src (time, lat, lon)\n",
    "            # list5 = [os.path.join(root, file) for file in sorted_files if file.split(\".\")[1] == 'taijlh1E33oma_ai']  # Aerosols Mixing Ratio (time, level, lat, lon)\n",
    "\n",
    "        # Convert `cftime.DatetimeNoLeap` to `pandas.to_datetime()`\n",
    "        warnings.filterwarnings(\"ignore\", message=\"Converting a CFTimeIndex with dates from a non-standard calendar\")\n",
    "        datetimeindex = xr.open_mfdataset(list1[:365]).indexes['time'].to_datetimeindex()\n",
    "\n",
    "        idx = np.arange(len(datetimeindex))\n",
    "        rng = np.random.default_rng(0)\n",
    "        rng.shuffle(idx)\n",
    "        \n",
    "        if   self.period == 'train':\n",
    "            self.datetimeindex = datetimeindex[idx[:12264]] # 70%\n",
    "        \n",
    "        elif self.period == 'val':\n",
    "            self.datetimeindex = datetimeindex[idx[12264:]] # 30%\n",
    "        \n",
    "        elif self.period == 'test':\n",
    "            self.datetimeindex = xr.open_mfdataset(list1[365:]).indexes['time'].to_datetimeindex()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        timestep = self.datetimeindex[index].strftime('%Y%m%d')\n",
    "        \n",
    "        ds1 = xr.open_dataset(os.path.join(self.root, f'{timestep}.aijlh1E33oma_ai.nc'))\n",
    "        ds1['time'] = ds1.indexes['time'].to_datetimeindex()\n",
    "        \n",
    "        ds2 = xr.open_dataset(os.path.join(self.root, f'{timestep}.cijh1E33oma_ai.nc'))\n",
    "        ds2['time'] = ds2.indexes['time'].to_datetimeindex()\n",
    "\n",
    "        X1 = np.expand_dims(ds1['u'].isel(level=0).sel(time=self.datetimeindex[index]), axis=0)\n",
    "        X2 = np.expand_dims(ds1['v'].isel(level=0).sel(time=self.datetimeindex[index]), axis=0)\n",
    "        X3 = np.expand_dims(ds1['omega'].isel(level=0).sel(time=self.datetimeindex[index]), axis=0)\n",
    "\n",
    "        X4 = np.expand_dims(ds2['prec'].sel(time=self.datetimeindex[index]), axis=0)\n",
    "\n",
    "        with open('variable_statistics.json', 'r') as jf:\n",
    "            vs = json.load(jf)\n",
    "        \n",
    "        X1_mean = vs['u']['mean'];    X1_std = vs['u']['std']\n",
    "        X2_mean = vs['v']['mean'];    X2_std = vs['v']['std']\n",
    "        X3_mean = vs['w']['mean'];    X3_std = vs['w']['std']\n",
    "        X4_mean = vs['prec']['mean']; X4_std = vs['prec']['std']\n",
    "\n",
    "        if self.species == 'seasalt':\n",
    "            # Add positive lag for target variable\n",
    "            ds3 = xr.open_dataset(os.path.join(self.root, f'{timestep}.taijh1E33oma_ai.nc'))\n",
    "            ds3['time'] = ds3.indexes['time'].to_datetimeindex()\n",
    "\n",
    "            ds4 = xr.open_dataset(os.path.join(self.root, f'{timestep}.taijlh1E33oma_ai.nc'))\n",
    "            ds4['time'] = ds4.indexes['time'].to_datetimeindex()\n",
    "\n",
    "            X5 = np.expand_dims(ds3['seasalt1_ocean_src'].sel(time=self.datetimeindex[index]), axis=0)\n",
    "            y  = np.expand_dims(ds4['seasalt1'].isel(level=0).sel(time=self.datetimeindex[index]), axis=0)\n",
    "\n",
    "            X5_mean = vs['ss_src']['mean']; X5_std = vs['ss_src']['std']\n",
    "            y_mean  = vs['ss_conc']['mean']; y_std = vs['ss_conc']['std']\n",
    "\n",
    "        if self.species == 'clay':\n",
    "            # Add positive lag for target variable\n",
    "            ds3 = xr.open_dataset(os.path.join(self.root, f'{timestep}.tNDaijh1E33oma_ai.nc'))\n",
    "            ds3['time'] = ds3.indexes['time'].to_datetimeindex()\n",
    "\n",
    "            ds4 = xr.open_dataset(os.path.join(self.root, f'{timestep}.taijlh1E33oma_ai.nc'))\n",
    "            ds4['time'] = ds4.indexes['time'].to_datetimeindex()\n",
    "\n",
    "            X5 = np.expand_dims(ds3['Clay_emission'].sel(time=self.datetimeindex[index]), axis=0)\n",
    "            y  = np.expand_dims(ds4['Clay'].isel(level=0).sel(time=self.datetimeindex[index]), axis=0)\n",
    "\n",
    "            X5_mean = vs['c_src']['mean']; X5_std = vs['c_src']['std']\n",
    "            y_mean  = vs['c_conc']['mean']; y_std = vs['c_conc']['std']\n",
    "\n",
    "        if self.species == 'bcb':\n",
    "            # Add positive lag for target variable\n",
    "            ds3 = xr.open_dataset(os.path.join(self.root, f'{timestep}.tNDaijh1E33oma_ai.nc'))\n",
    "            ds3['time'] = ds3.indexes['time'].to_datetimeindex()\n",
    "\n",
    "            ds4 = xr.open_dataset(os.path.join(self.root, f'{timestep}.taijlh1E33oma_ai.nc'))\n",
    "            ds4['time'] = ds4.indexes['time'].to_datetimeindex()\n",
    "\n",
    "            X5 = np.expand_dims(ds3['BCB_biomass_src'].sel(time=self.datetimeindex[index]), axis=0)\n",
    "            y  = np.expand_dims(ds4['BCB'].isel(level=0).sel(time=self.datetimeindex[index]), axis=0)\n",
    "\n",
    "            X5_mean = vs['bc_src']['mean']; X5_std = vs['bc_src']['std']\n",
    "            y_mean  = vs['bc_conc']['mean']; y_std = vs['bc_conc']['std']\n",
    "\n",
    "\n",
    "        X = np.concatenate((X1, X2, X3, X4, X5), axis=0)  # (5, 90, 144)\n",
    "\n",
    "        Xs_mean = np.array((X1_mean, X2_mean, X3_mean, X4_mean, X5_mean), dtype=np.float32).reshape(-1, 1, 1)\n",
    "        Xs_std  = np.array((X1_std, X2_std, X3_std, X4_std, X5_std), dtype=np.float32).reshape(-1, 1, 1)\n",
    " \n",
    "        self.y_mean = np.array(y_mean, dtype=np.float32).reshape(-1, 1, 1)\n",
    "        self.y_std  = np.array(y_std, dtype=np.float32).reshape(-1, 1, 1)\n",
    "\n",
    "        X = (X - Xs_mean) / Xs_std\n",
    "        y = (y -  self.y_mean) / self.y_std\n",
    "\n",
    "        if self.padding:\n",
    "            w = X.shape[2] # width\n",
    "            h = X.shape[1] # height\n",
    "            \n",
    "            top_pad   = self.padding - h\n",
    "            right_pad = self.padding - w\n",
    "            \n",
    "            X = np.lib.pad(X, ((0, 0), (top_pad, 0), (0, right_pad)), mode='constant', constant_values=0)\n",
    "        \n",
    "        X = torch.from_numpy(X).type(torch.float32) # torch image: C x H x W\n",
    "        y = torch.from_numpy(y).type(torch.float32) # torch image: C x H x W\n",
    "\n",
    "        return X, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.datetimeindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bfa4c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17520\n",
      "torch.Size([5, 256, 256]) torch.Size([1, 90, 144])\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", message=\"Converting a CFTimeIndex with dates from a non-standard calendar\")\n",
    "    \n",
    "    dataset = E33OMA(period='test', species='bcb', padding=256)\n",
    "\n",
    "    print(len(dataset))\n",
    "    dataiter = iter(dataset)\n",
    "    X, y = next(dataiter)\n",
    "    print(X.shape, y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
