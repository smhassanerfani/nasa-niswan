{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78d29a6d-8993-4286-87b4-c6541931351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6178414-fd97-4156-bc8d-8155e1fb2684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ab0d70-db81-47f9-a078-52ea36beb487",
   "metadata": {},
   "source": [
    "### Reading files\n",
    "The files' path for each `.nc` file is collected and stored in specific Python list, then the listed files are all read together. Eventually, they added to the first xarray dataset as new variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99149762-73c1-4abe-879f-f96bb6f0f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the lists of files\n",
    "\n",
    "for root, dirs, files in os.walk('/discover/nobackup/sebauer1/E33oma_ai/output/'):\n",
    "    \n",
    "    sorted_files = sorted(files)\n",
    "    ds_list1 = [os.path.join(root, file) for file in sorted_files if file.split(\".\")[1] == 'aijlh1E33oma_ai']\n",
    "    ds_list2 = [os.path.join(root, file) for file in sorted_files if file.split(\".\")[1] == 'taijlh1E33oma_ai']\n",
    "    ds_list3 = [os.path.join(root, file) for file in sorted_files if file.split(\".\")[1] in ['cijh1E33oma_ai', 'taijh1E33oma_ai']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2789fc1-689b-416b-8f90-47598f501d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the lists as NetCDF file\n",
    "\n",
    "ds1 = xr.open_mfdataset(ds_list1)\n",
    "ds1 = ds1.isel(level=0).drop_vars('level')\n",
    "ds1 = ds1.drop_vars(['axyp'])\n",
    "\n",
    "ds2 = xr.open_mfdataset(ds_list2)\n",
    "ds2 = ds2.isel(level=0).drop_vars('level')\n",
    "ds2 = ds2.drop_vars(['axyp', 'Clay', 'BCB'])\n",
    "\n",
    "ds3 = xr.open_mfdataset(ds_list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f692879b-c4d8-4f9e-8e64-eb0873bfe7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging variables in separate NetCDFs\n",
    "\n",
    "ds1['seasalt'] = ds2['seasalt1']\n",
    "ds1['prec'] = ds3['prec']\n",
    "ds1['seasalt_src'] = ds3['seasalt1_ocean_src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64c8c105-16fa-46bf-a564-ad3dfa5b26ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfsm/dnb33/tdirs/batch/slurm.30091577.serfani/ipykernel_37091/1578855123.py:3: RuntimeWarning: Converting a CFTimeIndex with dates from a non-standard calendar, 'noleap', to a pandas.DatetimeIndex, which uses dates from the standard calendar.  This may lead to subtle errors in operations that depend on the length of time between dates.\n",
      "  datetimeindex = ds1.indexes['time'].to_datetimeindex()\n"
     ]
    }
   ],
   "source": [
    "# Convert `cftime.DatetimeNoLeap` to `pandas.to_datetime()`\n",
    "\n",
    "datetimeindex = ds1.indexes['time'].to_datetimeindex()\n",
    "ds1['time'] = datetimeindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e7b17c-c8b7-48d3-bf7d-1c4ca98e3828",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1196d7-20ba-42c8-9800-0d4b3e662f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u: -30.785576 37.962673\n",
      "v: -32.26711 31.173237\n"
     ]
    }
   ],
   "source": [
    "print('u:', ds1['u'].values.min(), ds1['u'].values.max())\n",
    "print('v:', ds1['v'].values.min(), ds1['v'].values.max())\n",
    "print('omega:', ds1['omega'].values.min(), ds1['omega'].values.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c8719f-00ed-4c3b-b2d7-2b9136fdbe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cad9be0-2379-456a-ab27-e76e9e5367d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class E33OMA(Dataset):\n",
    "\n",
    "    def __init__(self, split, padding=None, joint_transform=None):\n",
    "        super(E33OMA, self).__init__()\n",
    "        \n",
    "        self.split = split\n",
    "        self.padding = padding\n",
    "        self.joint_transform = joint_transform\n",
    "        self.transform = T.ToTensor()\n",
    "        \n",
    "        self._get_data()\n",
    "    \n",
    "    \n",
    "    def _get_data(self):\n",
    "        \n",
    "        for root, dirs, files in os.walk('/discover/nobackup/sebauer1/E33oma_ai/output/'):\n",
    "\n",
    "            sorted_files = sorted(files)\n",
    "            ds_list1 = [os.path.join(root, file) for file in sorted_files if file.split(\".\")[1] == 'aijlh1E33oma_ai']\n",
    "            ds_list2 = [os.path.join(root, file) for file in sorted_files if file.split(\".\")[1] == 'taijlh1E33oma_ai']\n",
    "            ds_list3 = [os.path.join(root, file) for file in sorted_files if file.split(\".\")[1] in ['cijh1E33oma_ai', 'taijh1E33oma_ai']]\n",
    "\n",
    "        ds1 = xr.open_mfdataset(ds_list1)\n",
    "        ds1 = ds1.isel(level=0).drop_vars('level')\n",
    "        ds1 = ds1.drop_vars(['axyp'])\n",
    "\n",
    "        ds2 = xr.open_mfdataset(ds_list2)\n",
    "        ds2 = ds2.isel(level=0).drop_vars('level')\n",
    "        ds2 = ds2.drop_vars(['axyp', 'Clay', 'BCB'])\n",
    "\n",
    "        ds3 = xr.open_mfdataset(ds_list3)\n",
    "\n",
    "        ds1['seasalt'] = ds2['seasalt1']\n",
    "        ds1['prec'] = ds3['prec']\n",
    "        ds1['seasalt_src'] = ds3['seasalt1_ocean_src']\n",
    "\n",
    "        datetimeindex = ds1.indexes['time'].to_datetimeindex()\n",
    "        ds1['time'] = datetimeindex\n",
    "        \n",
    "        ds1.load()\n",
    "\n",
    "        # Add positive lag for target variable\n",
    "        target = np.expand_dims(ds1['seasalt'][1:, ...], axis=1) # (4319, 1, 90, 144)\n",
    "\n",
    "        # Add negative lag for input features\n",
    "        u = np.expand_dims(ds1['u'][:-1, ...], axis=1) # (4319, 1, 90, 144)\n",
    "        v = np.expand_dims(ds1['v'][:-1, ...], axis=1) # (4319, 1, 90, 144)\n",
    "        omega = np.expand_dims(ds1['omega'][:-1, ...], axis=1) # (4319, 1, 90, 144)\n",
    "        prec = np.expand_dims(ds1['prec'][:-1, ...], axis=1)   # (4319, 1, 90, 144)\n",
    "        seasalt_src = np.expand_dims(ds1['seasalt_src'][:-1, ...], axis=1) # (4319, 1, 90, 144)\n",
    "\n",
    "        features = np.concatenate((u, v, omega, prec, seasalt_src), axis=1) # (4319, 5, 90, 144)\n",
    "\n",
    "        \n",
    "        self.target_min = target[:3023, ...].min().reshape(-1, 1, 1)\n",
    "        self.target_max = target[:3023, ...].max().reshape(-1, 1, 1)\n",
    "\n",
    "        self.features_min = features[:3023, ...].min(axis=(0, 2, 3)).reshape(-1, 1, 1) \n",
    "        self.features_max = features[:3023, ...].max(axis=(0, 2, 3)).reshape(-1, 1, 1)\n",
    "        \n",
    "        if self.split == \"train\": # 70% of the total data\n",
    "            self.target = target[:3023, ...]\n",
    "            self.features = features[:3023, ...]\n",
    "        \n",
    "        elif self.split == \"val\": # 10% of the total data\n",
    "            self.target = target[3023:3455, ...]\n",
    "            self.features = features[3023:3455, ...]\n",
    "            \n",
    "        else: # (self.split == \"test\") # 20% of the total data\n",
    "            self.target = target[3455:, ...]\n",
    "            self.features = features[3455:, ...]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        X = self.features[index, ...]\n",
    "        y = self.target[index, ...]\n",
    "    \n",
    "        X = (X - self.features_min) / (self.features_max - self.features_min)\n",
    "        y = (y - self.target_min) / (self.target_max - self.target_min)\n",
    "\n",
    "        X = 2 * X - 1\n",
    "        y = 2 * y - 1\n",
    "        \n",
    "        if self.padding:\n",
    "            w = X.shape[2] # width\n",
    "            h = X.shape[1] # height\n",
    "            \n",
    "            top_pad   = self.padding - h\n",
    "            right_pad = self.padding - w\n",
    "            \n",
    "            X = np.lib.pad(X, ((0, 0), (top_pad, 0), (0, right_pad)), mode='constant', constant_values=0)\n",
    "            y = np.lib.pad(y, ((0, 0), (top_pad, 0), (0, right_pad)), mode='constant', constant_values=0)\n",
    "        \n",
    "        X = torch.from_numpy(X) # torch image: C x H x W\n",
    "        y = torch.from_numpy(y) # torch image: C x H x W\n",
    "            \n",
    "        if self.joint_transform:\n",
    "            X, y = self.joint_transform(X, y)\n",
    "\n",
    "        return X, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25fc9232-cd72-43e2-a3c4-25d73d0275c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfsm/dnb33/tdirs/batch/slurm.30282351.serfani/ipykernel_175681/2590725865.py:40: RuntimeWarning: Converting a CFTimeIndex with dates from a non-standard calendar, 'noleap', to a pandas.DatetimeIndex, which uses dates from the standard calendar.  This may lead to subtle errors in operations that depend on the length of time between dates.\n",
      "  datetimeindex = ds1.indexes['time'].to_datetimeindex()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 256, 256]) torch.Size([1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "dataset = E33OMA(split='val', padding=256)\n",
    "dataiter = iter(dataset)\n",
    "X, y = next(dataiter)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a55884-13a0-4857-bd03-606631724c33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-torch]",
   "language": "python",
   "name": "conda-env-.conda-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
