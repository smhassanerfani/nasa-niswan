{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        padding = (kernel_size - stride + 1) // 2\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=kernel_size,\n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.norm = nn.GroupNorm(2, out_channels)\n",
    "        self.act = nn.SiLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvTransposeLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super(ConvTransposeLayer, self).__init__()\n",
    "        padding = (kernel_size - stride + 1) // 2\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels * 4, kernel_size=kernel_size,\n",
    "                      stride=1, padding=padding),\n",
    "            nn.PixelShuffle(2)\n",
    "        )\n",
    "        self.norm = nn.GroupNorm(2, out_channels)\n",
    "        self.act = nn.SiLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super(Conv, self).__init__()\n",
    "        # Depth-wise convolution\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, \n",
    "                                   stride=stride, padding=padding, groups=in_channels)\n",
    "        # Point-wise convolution (1x1)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionMemory(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # attention for hidden layer\n",
    "        self.query_h = Conv(input_dim, hidden_dim, 1, padding=\"same\")\n",
    "        self.key_h = Conv(input_dim, hidden_dim, 1, padding=\"same\")\n",
    "        self.value_h = Conv(input_dim, input_dim, 1, padding=\"same\")\n",
    "        self.z_h = Conv(input_dim, input_dim, 1, padding=\"same\")\n",
    "\n",
    "        # attention for memory layer\n",
    "        self.key_m = Conv(input_dim, hidden_dim, 1, padding=\"same\")\n",
    "        self.value_m = Conv(input_dim, input_dim, 1, padding=\"same\")\n",
    "        self.z_m = Conv(input_dim, input_dim, 1, padding=\"same\")\n",
    "\n",
    "        # weights of concated channels of h Zh and Zm.\n",
    "        self.w_z = Conv(input_dim * 2, input_dim * 2, 1, padding=\"same\")\n",
    "\n",
    "        # weights of conated channels of Z and h.\n",
    "        self.w = Conv(input_dim * 3, input_dim * 3, 1, padding=\"same\")\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, h, m):\n",
    "        \"\"\"\n",
    "        Return:\n",
    "            Tuple(torch.Tensor, torch.Tensor): new Hidden layer and new memory module.\n",
    "        \"\"\"\n",
    "        batch_size, _, H, W = h.shape\n",
    "        # hidden attention\n",
    "        k_h = self.key_h(h)\n",
    "        q_h = self.query_h(h)\n",
    "        v_h = self.value_h(h)\n",
    "\n",
    "        k_h = k_h.view(batch_size, self.hidden_dim, H * W)\n",
    "        q_h = q_h.view(batch_size, self.hidden_dim, H * W).transpose(1, 2)\n",
    "        v_h = v_h.view(batch_size, self.input_dim, H * W)\n",
    "\n",
    "        attention_h = torch.softmax(torch.bmm(q_h, k_h), dim=-1)  # The shape is (batch_size, H*W, H*W)\n",
    "        z_h = torch.matmul(attention_h, v_h.permute(0, 2, 1))\n",
    "        z_h = z_h.transpose(1, 2).view(batch_size, self.input_dim, H, W)\n",
    "        z_h = self.z_h(z_h)\n",
    "\n",
    "        # memory attention\n",
    "        k_m = self.key_m(m)\n",
    "        v_m = self.value_m(m)\n",
    "\n",
    "        k_m = k_m.view(batch_size, self.hidden_dim, H * W)\n",
    "        v_m = v_m.view(batch_size, self.input_dim, H * W)\n",
    "\n",
    "        attention_m = torch.softmax(torch.bmm(q_h, k_m), dim=-1)\n",
    "        z_m = torch.matmul(attention_m, v_m.permute(0, 2, 1))\n",
    "        z_m = z_m.transpose(1, 2).view(batch_size, self.input_dim, H, W)\n",
    "        z_m = self.z_m(z_m)\n",
    "\n",
    "        # channel concat of Zh and Zm.\n",
    "        Z = torch.cat([z_h, z_m], dim=1)\n",
    "        Z = self.w_z(Z)\n",
    "\n",
    "        # channel concat of Z and h\n",
    "        W = torch.cat([Z, h], dim=1)\n",
    "        W = self.w(W)\n",
    "\n",
    "        # mi_conv: Wm; zi * Z + Wm; hi * Ht + bm; i\n",
    "        # mg_conv: Wm; zg * Z + Wm; hg * Ht + bm; g\n",
    "        # mo_conv: Wm; zo * Z + Wm; ho * Ht + bm; o\n",
    "        mi_conv, mg_conv, mo_conv = torch.chunk(W, chunks=3, dim=1)\n",
    "        input_gate = torch.sigmoid(mi_conv)\n",
    "        g = torch.tanh(mg_conv)\n",
    "        new_M = (1 - input_gate) * m + input_gate * g\n",
    "        output_gate = torch.sigmoid(mo_conv)\n",
    "        new_H = output_gate * new_M\n",
    "\n",
    "        return new_H, new_M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels = 5, patch_size = 16, emb_size = 128):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            # break-down the image in s1 x s2 patches and flat them\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n",
    "            nn.Linear(patch_size * patch_size * in_channels, emb_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "class PatchUnembedding(nn.Module):\n",
    "    def __init__(self, in_channels=5, patch_size=16, emb_size=128, img_size=160):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.img_size = img_size\n",
    "        self.emb_size = emb_size\n",
    "        self.in_channels = in_channels\n",
    "        self.reconstruction = nn.Sequential(\n",
    "            nn.Linear(emb_size, patch_size * patch_size * in_channels),\n",
    "            Rearrange('b (h w) (p1 p2 c) -> b c (h p1) (w p2)', \n",
    "                      p1=patch_size, p2=patch_size, h=img_size // patch_size, w=img_size // patch_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.reconstruction(x)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.att = torch.nn.MultiheadAttention(embed_dim=dim,\n",
    "                                               num_heads=n_heads,\n",
    "                                               dropout=dropout)\n",
    "        self.q = torch.nn.Linear(dim, dim)\n",
    "        self.k = torch.nn.Linear(dim, dim)\n",
    "        self.v = torch.nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x, q=None):\n",
    "        if q is None:\n",
    "            q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        attn_output, attn_output_weights = self.att(q, k, v)\n",
    "        return attn_output, q\n",
    "\n",
    "\n",
    "class SelfAttentionMemory(nn.Module):\n",
    "    def __init__(self, image_size: int, in_channels: int, patch_size: int, emb_size: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.input_dim = in_channels\n",
    "        self.patch_size = patch_size\n",
    "        self.emb_size = emb_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        self.patch_embed_h = PatchEmbedding(in_channels, patch_size, emb_size)\n",
    "        self.patch_embed_m = PatchEmbedding(in_channels, patch_size, emb_size)\n",
    "\n",
    "        self.pos_embedding_h = nn.Parameter(torch.randn(1, self.num_patches, emb_size))\n",
    "        self.pos_embedding_m = nn.Parameter(torch.randn(1, self.num_patches, emb_size))\n",
    "\n",
    "        self.attention_h = Attention(emb_size, 4, 0.1)\n",
    "        self.attention_m = Attention(emb_size, 4, 0.1)\n",
    "\n",
    "        self.patch_unembed_h = PatchUnembedding(in_channels, patch_size, emb_size, image_size)\n",
    "        self.patch_unembed_m = PatchUnembedding(in_channels, patch_size, emb_size, image_size)\n",
    "\n",
    "\n",
    "        # attention for hidden layer\n",
    "        self.z_h = Conv(in_channels, in_channels, 1, padding=\"same\")\n",
    "        self.z_m = Conv(in_channels, in_channels, 1, padding=\"same\")\n",
    "\n",
    "        # weights of concated channels of h Zh and Zm.\n",
    "        self.w_z = Conv(in_channels * 2, in_channels * 2, 1, padding=\"same\")\n",
    "\n",
    "        # weights of conated channels of Z and h.\n",
    "        self.w = Conv(in_channels * 3, in_channels * 3, 1, padding=\"same\")\n",
    "\n",
    "    def forward(self, h, m):\n",
    "        \"\"\"\n",
    "        Return:\n",
    "            Tuple(torch.Tensor, torch.Tensor): new Hidden layer and new memory module.\n",
    "        \"\"\"\n",
    "\n",
    "        z_h = self.patch_embed_h(h)\n",
    "        z_m = self.patch_embed_m(m)\n",
    "\n",
    "        z_h = z_h + self.pos_embedding_h\n",
    "        z_m = z_m + self.pos_embedding_m\n",
    "\n",
    "        z_h, q_h = self.attention_h(z_h)\n",
    "        z_m, _ = self.attention_m(z_m, q_h)\n",
    "\n",
    "        z_h = self.patch_unembed_h(z_h)\n",
    "        z_m = self.patch_unembed_m(z_m)\n",
    "\n",
    "        z_h = self.z_h(z_h)\n",
    "        z_m = self.z_m(z_m)\n",
    "\n",
    "        # channel concat of Zh and Zm.\n",
    "        Z = torch.cat([z_h, z_m], dim=1)\n",
    "        Z = self.w_z(Z)\n",
    "\n",
    "        # channel concat of Z and h\n",
    "        W = torch.cat([Z, h], dim=1)\n",
    "        W = self.w(W)\n",
    "\n",
    "        # mi_conv: Wm; zi * Z + Wm; hi * Ht + bm; i\n",
    "        # mg_conv: Wm; zg * Z + Wm; hg * Ht + bm; g\n",
    "        # mo_conv: Wm; zo * Z + Wm; ho * Ht + bm; o\n",
    "        mi_conv, mg_conv, mo_conv = torch.chunk(W, chunks=3, dim=1)\n",
    "        \n",
    "        input_gate = torch.sigmoid(mi_conv)\n",
    "        g = torch.tanh(mg_conv)\n",
    "        new_M = (1 - input_gate) * m + input_gate * g\n",
    "        \n",
    "        output_gate = torch.sigmoid(mo_conv)\n",
    "        new_H = output_gate * new_M\n",
    "\n",
    "        return new_H, new_M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = ConvLayer(in_channels,  out_channels, kernel_size, stride=1)\n",
    "        self.conv2 = ConvLayer(out_channels, out_channels, kernel_size, stride=2)\n",
    "        self.conv3 = ConvLayer(out_channels, out_channels, kernel_size, stride=1)\n",
    "        self.conv4 = ConvLayer(out_channels, out_channels, kernel_size, stride=2)\n",
    "    \n",
    "    def forward(self, x):  # BxT, 5, 160, 160\n",
    "        enc1 = self.conv1(x)\n",
    "        x = self.conv2(enc1)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        return x, enc1\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, hid_channels, out_channels, kernel_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv1 = ConvTransposeLayer(hid_channels, hid_channels, kernel_size, stride=1)\n",
    "        self.conv2 = ConvLayer(hid_channels, hid_channels, kernel_size, stride=1)\n",
    "        self.conv3 = ConvTransposeLayer(hid_channels, hid_channels, kernel_size, stride=1)\n",
    "        self.conv4 = ConvLayer(hid_channels, hid_channels, kernel_size, stride=1)\n",
    "        self.readout = nn.Conv2d(hid_channels, out_channels, kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, x, enc1):  # BxT, 5, 40, 40\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x + enc1)\n",
    "        x = self.readout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size, bias=True):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.W_ci = nn.parameter.Parameter(\n",
    "            torch.zeros(hidden_channels, 160, 160, dtype=torch.float)\n",
    "        )\n",
    "        self.W_co = nn.parameter.Parameter(\n",
    "            torch.zeros(hidden_channels, 160, 160, dtype=torch.float)\n",
    "        )\n",
    "        self.W_cf = nn.parameter.Parameter(\n",
    "            torch.zeros(hidden_channels, 160, 160, dtype=torch.float)\n",
    "        )\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_channels + self.hidden_channels,\n",
    "                              out_channels=4 * self.hidden_channels,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "        \n",
    "        self.attention_memory = SelfAttentionMemory(image_size=160, in_channels=self.hidden_channels, patch_size=16, emb_size=128)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, hidden_state):\n",
    "        h, c, m = hidden_state\n",
    "\n",
    "        combined = torch.cat([x, h], dim=1)\n",
    "        gates = self.conv(combined)\n",
    "        ingate, forgetgate, cellgate, outgate  = torch.split(gates, self.hidden_channels, dim=1)\n",
    "        \n",
    "        ingate     = self.sigmoid(ingate + self.W_ci * c)\n",
    "        forgetgate = self.sigmoid(forgetgate + self.W_cf * c)\n",
    "        cellgate   = self.tanh(cellgate)\n",
    "        # outgate    = self.sigmoid(outgate)\n",
    "\n",
    "        c = c * forgetgate + ingate * cellgate\n",
    "        \n",
    "        outgate = self.sigmoid(outgate + self.W_co * c)\n",
    "        h = outgate * self.tanh(c)\n",
    "        h, m = self.attention_memory(h, m)\n",
    "        return h, c, m\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self.num_layers = len(hidden_channels)\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        # Create a list of ConvLSTM cells\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Add the first layer\n",
    "        self.layers.append(ConvLSTMCell(input_channels, hidden_channels[0], kernel_size[0]))\n",
    "        \n",
    "        # Add subsequent layers\n",
    "        for i in range(1, self.num_layers):\n",
    "            self.layers.append(ConvLSTMCell(hidden_channels[i-1], hidden_channels[i], kernel_size[i]))\n",
    "\n",
    "        # Bottleneck layer\n",
    "        self.conv  = nn.Conv2d(hidden_channels[-1], 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assuming x is a sequence of frames: (batch_size, sequence_length, channels, height, width)\n",
    "        batch_size, seq_len, _, height, width = x.size()\n",
    "        \n",
    "        # Initialize hidden and cell states for each layer\n",
    "        hidden_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            h = torch.zeros(batch_size, self.hidden_channels[i], height, width).to(x.device)\n",
    "            c = torch.zeros(batch_size, self.hidden_channels[i], height, width).to(x.device)\n",
    "            m = torch.zeros(batch_size, self.hidden_channels[i], height, width).to(x.device)\n",
    "            hidden_states.append((h, c, m))\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :, :, :]\n",
    "            for layer_idx in range(self.num_layers):\n",
    "                h, c, m = hidden_states[layer_idx]\n",
    "                h, c, m = self.layers[layer_idx](x_t, (h, c, m))\n",
    "                hidden_states[layer_idx] = (h, c, m)\n",
    "                x_t = h  # Output of the current layer is the input to the next layer\n",
    "\n",
    "        return self.conv(h).unsqueeze(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 1, 160, 160])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((2, 48, 5, 160, 160))\n",
    "model = ConvLSTM(5, [64, 32, 16], [5, 3, 3])\n",
    "print(model(x).shape)  # (2, 1, 16, 160, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttention(nn.Module):\n",
    "    \"\"\"A Temporal Attention block for Temporal Attention Unit\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, kernel_size=21, attn_shortcut=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj_1 = nn.Conv2d(d_model, d_model, 1)         # 1x1 conv\n",
    "        self.activation = nn.GELU()                          # GELU\n",
    "        self.spatial_gating_unit = TemporalAttentionModule(d_model, kernel_size)\n",
    "        self.proj_2 = nn.Conv2d(d_model, d_model, 1)         # 1x1 conv\n",
    "        self.attn_shortcut = attn_shortcut\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.attn_shortcut:\n",
    "            shortcut = x.clone()\n",
    "        x = self.proj_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.spatial_gating_unit(x)\n",
    "        x = self.proj_2(x)\n",
    "        if self.attn_shortcut:\n",
    "            x = x + shortcut\n",
    "        return x\n",
    "    \n",
    "\n",
    "class TemporalAttentionModule(nn.Module):\n",
    "    \"\"\"Large Kernel Attention for SimVP\"\"\"\n",
    "\n",
    "    def __init__(self, dim, kernel_size, dilation=3, reduction=16):\n",
    "        super().__init__()\n",
    "        d_k = 2 * dilation - 1\n",
    "        d_p = (d_k - 1) // 2\n",
    "        dd_k = kernel_size // dilation + ((kernel_size // dilation) % 2 - 1)\n",
    "        dd_p = (dilation * (dd_k - 1) // 2)\n",
    "\n",
    "        self.conv0 = nn.Conv2d(dim, dim, d_k, padding=d_p, groups=dim)\n",
    "        self.conv_spatial = nn.Conv2d(\n",
    "            dim, dim, dd_k, stride=1, padding=dd_p, groups=dim, dilation=dilation)\n",
    "        self.conv1 = nn.Conv2d(dim, dim, 1)\n",
    "\n",
    "        self.reduction = max(dim // reduction, 4)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(dim, dim // self.reduction, bias=False), # reduction\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(dim // self.reduction, dim, bias=False), # expansion\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.clone()\n",
    "        attn = self.conv0(x)           # depth-wise conv\n",
    "        attn = self.conv_spatial(attn) # depth-wise dilation convolution\n",
    "        f_x = self.conv1(attn)         # 1x1 conv\n",
    "        # append a se operation\n",
    "        b, c, _, _ = x.size()\n",
    "        se_atten = self.avg_pool(x).view(b, c)\n",
    "        se_atten = self.fc(se_atten).view(b, c, 1, 1)\n",
    "        return se_atten * f_x * u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 768, 40, 40])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((8, 48*16, 40, 40))\n",
    "model = TemporalAttention(48*16)\n",
    "print(model(x).shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncDecConvLSTM(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
    "        super(EncDecConvLSTM, self).__init__()\n",
    "        self.encoder = Encoder(input_channels, hidden_channels, kernel_size)\n",
    "        self.decoder = Decoder(hidden_channels, 1, kernel_size)\n",
    "        # self.lstm = ConvLSTM(hidden_channels, [hidden_channels, hidden_channels, hidden_channels], [5, 3, 3])\n",
    "        self.attn = TemporalAttention(hidden_channels*48)\n",
    "        self.conv = nn.Conv2d(48, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.size()\n",
    "        x = x.view(B*T, C, H, W)\n",
    "\n",
    "        z, enc1 = self.encoder(x)\n",
    "\n",
    "        C_new, H_new, W_new = z.size(1), z.size(2), z.size(3)\n",
    "        z = z.view(B, T, C_new, H_new, W_new) # 8, 48, 16, 40, 40\n",
    "\n",
    "        # z = self.lstm(z)\n",
    "        z = z.view(B, T*C_new, H_new, W_new) \n",
    "        z = self.attn(z)\n",
    "\n",
    "        # T_new = z.size(1)\n",
    "        z = z.view(B, T, C_new, H_new, W_new) # 8, 48, 16, 40, 40\n",
    "        z = z.view(B*T, C_new, H_new, W_new)\n",
    "\n",
    "        out = self.decoder(z, enc1)\n",
    "        out = out.view(B, T, H, W)\n",
    "        out = self.conv(out).unsqueeze(2) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 1, 160, 160])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((8, 48, 5, 160, 160))\n",
    "model = EncDecConvLSTM(5, 16, 3)\n",
    "\n",
    "y = model(x)\n",
    "print(y.size())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lightning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
